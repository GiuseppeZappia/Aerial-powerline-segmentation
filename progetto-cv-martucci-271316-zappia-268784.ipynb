{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14283719,"sourceType":"datasetVersion","datasetId":9116887},{"sourceId":14283832,"sourceType":"datasetVersion","datasetId":9116964},{"sourceId":14413579,"sourceType":"datasetVersion","datasetId":9116980},{"sourceId":14488590,"sourceType":"datasetVersion","datasetId":9245186}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Rilevamento e Segmentazione di Cavi Elettrici su Dataset TTPLA\n## Implementazione basata su Detectron2 e PointRend con Loss Ibrida\n\n**Corso:** Computer Vision - A.A. 2025/2026  \n**Università:** Università della Calabria (DIMES)  \n**Studenti:** Martucci Anastasia (271316), Zappia Giuseppe (268784)  \n**Docenti:** Prof. Manco Giuseppe, Prof. Pisani Francesco Sergio  \n\n---\n\n### Introduzione e Obiettivi del Task\nL'obiettivo di questo progetto è lo sviluppo di un modello di **Instance Segmentation** capace di individuare cavi elettrici in scenari aerei complessi utilizzando il dataset **TTPLA**. \nIl task presenta sfide significative:\n1. **Sottigliezza estrema:** i cavi occupano spesso solo 1-3 pixel.\n2. **Sbilanciamento delle classi:** il background domina l'immagine (>99%).\n3. **Discontinuità:** le reti standard tendono a frammentare i cavi a causa del downsampling.\n\nIl modello finale deve restituire: score, bounding box, maschera di segmentazione e coordinate polari $(\\rho, \\theta)$ della retta.","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup dell'Ambiente e Installazione\nInstalliamo le dipendenze necessarie, tra cui **Detectron2** (sviluppato da FAIR) e le utility per la gestione del formato COCO.","metadata":{}},{"cell_type":"code","source":"# Installazione di Detectron2\n!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n!pip install pycocotools opencv-python-headless","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\nimport numpy as np\nimport os, json, cv2, random\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor, DefaultTrainer\nfrom detectron2.config import get_cfg\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nimport albumentations as A\nfrom tqdm import tqdm\nimport copy\n\n# --- PERCORSI DI INPUT  ---\nPATH_IMG_TRAIN = \"/kaggle/input/train-cv\" \nPATH_JSON_TRAIN = \"/kaggle/input/json-annotazioni/train.json\"\n\n# --- PERCORSO DI OUTPUT ---\nTRAIN_AUG_DIR = \"/kaggle/working/train_aug_dataset\"\nTRAIN_AUG_IMG_DIR = os.path.join(TRAIN_AUG_DIR, \"images\")\nTRAIN_AUG_JSON_PATH = os.path.join(TRAIN_AUG_DIR, \"train_aug.json\")\n\nos.makedirs(TRAIN_AUG_IMG_DIR, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Strategia di Data Augmentation Offline\nDato il numero ridotto di campioni (842 immagini di training), abbiamo implementato una pipeline di **Augmentation Offline**.\nAbbiamo scelto trasformazioni **geometriche non distruttive** (rotazioni di 90°, flip) per aumentare il dataset di 5 volte senza introdurre artefatti di interpolazione che potrebbero danneggiare la definizione dei cavi sottili.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os, json, cv2, copy\nimport albumentations as A\nfrom tqdm import tqdm\n\ndef geometric_augmentation(inputs_list, output_img_dir, output_json_path, multiplier=5):\n    \"\"\"\n    Genera il dataset aumentato SOLO GEOMETRICAMENTE\n    \"\"\"\n    \n    # Contatori globali\n    global_img_id = 1\n    global_ann_id = 1\n    \n    final_images = []\n    final_annotations = []\n    \n    # Categoria (id 0 = cavo)\n    categories = [{\"id\": 0, \"name\": \"cable\"}] \n\n    # --- PIPELINE DI AUGMENTATION ---\n    transform = A.Compose([\n        A.HorizontalFlip(p=0.7),\n        A.VerticalFlip(p=0.7),\n        A.RandomRotate90(p=0.7),\n        A.Transpose(p=0.7),\n    ])\n\n    print(f\"Inizio augmentation geometrica dataset in: {output_img_dir}\")\n\n    for json_path, img_source_dir in inputs_list:\n        \n        with open(json_path) as f:\n            data = json.load(f)\n            \n        # Mappa annotazioni per ID immagine\n        img_to_anns = {img['id']: [] for img in data['images']}\n        if 'annotations' in data:\n            for ann in data['annotations']:\n                img_to_anns[ann['image_id']].append(ann)\n\n        for img_info in tqdm(data['images']):\n            src_path = os.path.join(img_source_dir, img_info['file_name'])\n            \n            if not os.path.exists(src_path):\n                continue\n                \n            image = cv2.imread(src_path)\n            # Conversione in RGB per Albumentations\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            h, w = image.shape[:2]\n            \n            anns = img_to_anns.get(img_info['id'], [])\n            \n            # --- FASE 1: Preparazione Maschere Separate ---           \n            masks_list = []\n            if len(anns) > 0:\n                for idx, ann in enumerate(anns):\n                    mask = np.zeros((h, w), dtype=np.uint8)\n                    for seg in ann['segmentation']:\n                        poly = np.array(seg).reshape((-1, 2)).astype(np.int32)\n                        cv2.fillPoly(mask, [poly], 1)\n                    masks_list.append(mask)\n            \n            # --- FASE 2: Generazione Versioni ---          \n            for i in range(multiplier):\n                filename_base = \"\"\n                \n                # Caso 0: Immagine Originale\n                if i == 0:\n                    augmented_image = image\n                    augmented_masks = masks_list\n                    filename_base = f\"img_{global_img_id}_orig.jpg\"\n                \n                # Caso > 0: Augmentation (Solo Geometrica)\n                else:\n                    if len(masks_list) > 0:\n                        augmented = transform(image=image, masks=masks_list)\n                        augmented_image = augmented['image']\n                        augmented_masks = augmented['masks']\n                    else:\n                        augmented = transform(image=image)\n                        augmented_image = augmented['image']\n                        augmented_masks = []\n                    \n                    filename_base = f\"img_{global_img_id}_aug_{i}.jpg\"\n\n                # Salvataggio immagine su disco (tornando in BGR per opencv)\n                save_path = os.path.join(output_img_dir, filename_base)\n                cv2.imwrite(save_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n                \n                # Aggiunta entry Immagine al JSON\n                new_img_entry = {\n                    \"id\": global_img_id,\n                    \"file_name\": filename_base,\n                    \"height\": h,\n                    \"width\": w\n                }\n                final_images.append(new_img_entry)\n                \n                # --- FASE 3: Ricostruzione Annotazioni COCO ---\n                for mask_idx, aug_mask in enumerate(augmented_masks):\n                    if np.sum(aug_mask) == 0:\n                        continue\n                        \n                    contours, _ = cv2.findContours(aug_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n                    new_segmentation = []\n                    total_area = 0\n                    \n                    for contour in contours:\n                        if cv2.contourArea(contour) < 5: \n                            continue\n                        seg_coords = contour.flatten().tolist()\n                        if len(seg_coords) >= 6:\n                            new_segmentation.append(seg_coords)\n                            total_area += cv2.contourArea(contour)\n                    \n                    if len(new_segmentation) > 0:\n                        ys, xs = np.where(aug_mask > 0)\n                        x_min, x_max = xs.min(), xs.max()\n                        y_min, y_max = ys.min(), ys.max()\n                        w_box = x_max - x_min\n                        h_box = y_max - y_min\n                        \n                        new_ann = {\n                            \"id\": global_ann_id,\n                            \"image_id\": global_img_id,\n                            \"category_id\": 0,\n                            \"segmentation\": new_segmentation,\n                            \"area\": float(total_area),\n                            \"bbox\": [float(x_min), float(y_min), float(w_box), float(h_box)],\n                            \"iscrowd\": 0\n                        }\n                        final_annotations.append(new_ann)\n                        global_ann_id += 1\n                \n                global_img_id += 1\n\n    # Salvataggio JSON aumentato\n    final_json = {\n        \"images\": final_images,\n        \"annotations\": final_annotations,\n        \"categories\": categories\n    }\n    \n    with open(output_json_path, 'w') as f:\n        json.dump(final_json, f)\n    \n    print(f\"Totale immagini: {len(final_images)}, Totale annotazioni: {len(final_annotations)}\")\n\ntraining_inputs = [(PATH_JSON_TRAIN, PATH_IMG_TRAIN)]\n\ngeometric_augmentation(training_inputs, TRAIN_AUG_IMG_DIR, TRAIN_AUG_JSON_PATH, multiplier=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- VISUALIZZAZIONE DI CONTROLLO DELL'AUGMENTATION ---\nimport matplotlib.pyplot as plt\nimport random\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.data.datasets import register_coco_instances\n\n# 1. Registriamo il dataset per la visualizzazione\nAUG_VIS_DATASET_NAME = \"vis_debug_augmented_v2\"\n\n# Pulizia preventiva se il dataset è già registrato\nif AUG_VIS_DATASET_NAME in DatasetCatalog.list():\n    DatasetCatalog.remove(AUG_VIS_DATASET_NAME)\n    MetadataCatalog.remove(AUG_VIS_DATASET_NAME)\n\nregister_coco_instances(AUG_VIS_DATASET_NAME, {}, TRAIN_AUG_JSON_PATH, TRAIN_AUG_IMG_DIR)\n\n# 2. Impostiamo i metadati gestendo il mapping degli ID \naug_metadata = MetadataCatalog.get(AUG_VIS_DATASET_NAME)\naug_metadata.set(\n    thing_classes=[\"cable\"],\n    # Esplicitiamo che l'ID 0 del JSON corrisponde alla classe 0 interna\n    thing_dataset_id_to_contiguous_id={0: 0} \n)\n\n# 3. Carichiamo il dataset\ndataset_dicts = DatasetCatalog.get(AUG_VIS_DATASET_NAME)\n\n# 4. Selezione CASUALE di 5 immagini\nnum_to_show = 5\n# Usiamo random.sample per prenderne 5 diverse a caso dal totale\nsamples = random.sample(dataset_dicts, num_to_show)\n\nprint(f\"Selezioniamo casualmente {num_to_show} immagini dal dataset aumentato per verificare le annotazioni.\")\n\n# Impostiamo una figura alta e stretta per la disposizione a colonna\nplt.figure(figsize=(10, 25)) \n\nfor i, d in enumerate(samples):\n    img = cv2.imread(d[\"file_name\"])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    visualizer = Visualizer(img, metadata=aug_metadata, scale=1.0)\n    \n    # Disegniamo le annotazioni\n    vis = visualizer.draw_dataset_dict(d)\n    \n    plt.subplot(num_to_show, 1, i + 1)\n    \n    plt.imshow(vis.get_image())\n    \n    file_name = os.path.basename(d[\"file_name\"])\n    plt.title(f\"File: {file_name} | ID: {d['image_id']}\", fontsize=12)\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Abbiamo verificato che le maschere siano coerenti anche su campioni casuali del dataset.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Architettura Custom: PointRend & Loss Ibrida\nPer superare i limiti della griglia fissa di Mask R-CNN ($28 \\times 28$), adottiamo **PointRend**. Questa architettura tratta la segmentazione come un problema di rendering, calcolando i pixel in modo adattivo solo dove l'incertezza è alta (sui bordi).\n\n### Loss Ibrida Focal + Dice\nPer gestire il forte sbilanciamento delle classi, implementiamo una funzione di perdita custom:\n$$L_{total} = 5 \\cdot L_{focal} + L_{dice}$$\n\n* **Focal Loss:** Costringe la rete a concentrarsi sui pixel \"difficili\" (i cavi) ignorando lo sfondo facile.\n* **Dice Loss:** Ottimizza la sovrapposizione globale, garantendo la continuità lineare del cavo.","metadata":{}},{"cell_type":"code","source":"# 1. IMPLEMENTAZIONE CUSTOM LOSS (DICE + FOCAL) E CUSTOM MASK HEAD \nimport torch\nfrom torch.nn import functional as F\nfrom torchvision.ops import sigmoid_focal_loss\nfrom detectron2.projects.point_rend.point_head import StandardPointHead, POINT_HEAD_REGISTRY\nfrom detectron2.projects.point_rend import PointRendMaskHead\nfrom detectron2.modeling import ROI_MASK_HEAD_REGISTRY\nfrom detectron2.modeling.roi_heads.mask_head import mask_rcnn_loss\n\ndef dice_loss(inputs, targets, smooth=1.0):\n    \"\"\"\n    Calcola la Dice Loss per segmentazione binaria.\n    \"\"\"\n    inputs = inputs.view(-1)\n    targets = targets.view(-1)\n    intersection = (inputs * targets).sum()                            \n    dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n    return 1 - dice\n\n# --- DEFINIZIONE CLASSI ---\n\nclass CustomCablePointHead(StandardPointHead):\n    \"\"\"\n    Versione modificata della PointHead per TTPLA.\n    Usa DiceLoss + FocalLoss.\n    \"\"\"\n    def losses(self, point_logits, point_labels):\n        if point_logits.dim() == 3 and point_logits.shape[1] == 1:\n            point_logits = point_logits.squeeze(1)\n        \n        point_probs = point_logits.sigmoid()\n        target = point_labels.float()\n\n        # 1. FOCAL LOSS\n        loss_focal = sigmoid_focal_loss(\n            point_logits, \n            target, \n            alpha=0.95, \n            gamma=2.0, \n            reduction=\"mean\"\n        )\n\n        # 2. DICE LOSS\n        loss_dice = dice_loss(point_probs, target)\n\n        # 3. Combinazione\n        total_loss = loss_dice + (loss_focal * 5.0)\n\n        return {\"loss_point\": total_loss}\n\nclass CustomCableMaskHead(PointRendMaskHead):\n    \"\"\"\n    MaskHead personalizzata che chiama la loss custom.\n    \"\"\"\n    def forward(self, features, instances):\n        if self.training:\n            proposal_boxes = [x.proposal_boxes for x in instances]\n            \n            # 1. Coarse Mask\n            coarse_mask = self.coarse_head(self._roi_pooler(features, proposal_boxes))\n            losses = {\"loss_mask\": mask_rcnn_loss(coarse_mask, instances)}\n            \n            if not self.mask_point_on:\n                return losses\n\n            # 2. Point Sampling\n            point_coords, point_labels = self._sample_train_points(coarse_mask, instances)\n            \n            # 3. Features\n            point_fine_grained_features = self._point_pooler(features, proposal_boxes, point_coords)\n            \n            # 4. Logits\n            point_logits = self._get_point_logits(\n                point_fine_grained_features, point_coords, coarse_mask\n            )\n            \n            # 5. Custom Loss\n            point_loss_dict = self.point_head.losses(point_logits, point_labels)\n            losses.update(point_loss_dict)\n            \n            return losses\n        else:\n            pred_boxes = [x.pred_boxes for x in instances]\n            coarse_mask = self.coarse_head(self._roi_pooler(features, pred_boxes))\n            return self._subdivision_inference(features, coarse_mask, instances)\n\n# --- REGISTRAZIONE SICURA ---\n\nif \"CustomCablePointHead\" in POINT_HEAD_REGISTRY:\n    del POINT_HEAD_REGISTRY._obj_map[\"CustomCablePointHead\"]\nPOINT_HEAD_REGISTRY.register(CustomCablePointHead)\n\nif \"CustomCableMaskHead\" in ROI_MASK_HEAD_REGISTRY:\n    del ROI_MASK_HEAD_REGISTRY._obj_map[\"CustomCableMaskHead\"]\nROI_MASK_HEAD_REGISTRY.register(CustomCableMaskHead)\n\nprint(\"Classi registrate correttamente.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cloniamo il repo solo per avere i file di config (yaml) a portata di mano\n!git clone https://github.com/facebookresearch/detectron2.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Addestramento del Modello\nConfiguriamo il training seguendo la **3x Schedule** di Detectron2 (~43 epoche).\nUtilizziamo una backbone **ResNet-50** con **FPN** (Feature Pyramid Network) per estrarre caratteristiche multiscala. In questa fase applichiamo anche una **Augmentation Online** fotometrica (luminosità, contrasto) per migliorare la robustezza.","metadata":{}},{"cell_type":"code","source":"# --- IMPORT NECESSARI ---\n!pip install gdown\nfrom detectron2.projects import point_rend\nimport detectron2.data.transforms as T\nfrom detectron2.data import detection_utils as utils\nimport os\nimport gdown\nimport torch\nimport copy\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\n\n# --- 1. DOWNLOAD PESI ---\nfile_id = '1SoFg6AjB17CIekGvAf_sLIuCE7wEmVfK'\noutput_weights = \"pointrend_rcnn_R_50_FPN_3x_model_final_3c3198.pkl\"\nurl = f'https://drive.google.com/uc?id={file_id}'\n\nif not os.path.exists(output_weights):\n    print(f\"Scaricamento pesi da mirror Google Drive...\")\n    gdown.download(url, output_weights, quiet=False)\nelse:\n    print(\"File dei pesi già presente.\")\n\n# --- 2. SETUP DATASET ---\ndataset_train_name = \"training_dataset_aug\"\nDatasetCatalog.clear()\n\ntry:\n    DatasetCatalog.register(dataset_train_name, lambda: detectron2.data.datasets.load_coco_json(TRAIN_AUG_JSON_PATH, TRAIN_AUG_IMG_DIR))\n    MetadataCatalog.get(dataset_train_name).set(thing_classes=[\"cable\"])\n    print(f\"Dataset {dataset_train_name} registrato correttamente.\")\nexcept Exception as e:\n    print(f\"Errore registrazione dataset: {e}\")\n\n# --- 3. MAPPER (FOTOMETRIA + FORMATO) ---\ndef custom_mapper(dataset_dict):\n    \"\"\"\n    Gestisce Fotometria (Online) e Formato.\n    \"\"\"\n    dataset_dict = copy.deepcopy(dataset_dict)\n    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n    \n    # Lista trasformazioni SOLO FOTOMETRICHE + RESIZE\n    transform_list = [\n        # --- FOTOMETRIA ---\n        T.RandomBrightness(0.8, 1.2),\n        T.RandomContrast(0.8, 1.2),\n        T.RandomSaturation(0.8, 1.2),\n        T.RandomLighting(0.7),\n        \n        # --- FORMATO  ---\n        # Invece di fisso (700, 700), diamo un range. \n        # Il modello imparerà a vedere i cavi a diverse risoluzioni.\n        T.ResizeShortestEdge(\n            short_edge_length=(640, 672, 704, 736, 768, 800), \n            max_size=1333, \n            sample_style='choice'\n        )\n    ]\n    \n    image, transforms = T.apply_transform_gens(transform_list, image)\n    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n    \n    annos = [\n        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n        for obj in dataset_dict.pop(\"annotations\")\n        if obj.get(\"iscrowd\", 0) == 0\n    ]\n    \n    # --- FORMATO (Bitmask per PointRend) ---\n    instances = utils.annotations_to_instances(annos, image.shape[:2], mask_format=\"bitmask\")\n    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n    return dataset_dict\n\nclass CustomTrainer(DefaultTrainer):\n    @classmethod\n    def build_train_loader(cls, cfg):\n        return detectron2.data.build_detection_train_loader(cfg, mapper=custom_mapper)\n\n# --- 4. CONFIGURAZIONE COMPLETA ---\ncfg = get_cfg()\npoint_rend.add_pointrend_config(cfg)\nconfig_path = \"detectron2/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\"\ncfg.merge_from_file(config_path)\n\ncfg.DATASETS.TRAIN = (dataset_train_name,)\ncfg.DATASETS.TEST = () \ncfg.MODEL.WEIGHTS = output_weights\n\n# === VOGLIAMO USARE LA NOSTRA LOSS ===\ncfg.MODEL.ROI_MASK_HEAD.NAME = \"CustomCableMaskHead\" \ncfg.MODEL.POINT_HEAD.NAME = \"CustomCablePointHead\" \n\n# Parametri Point Head\ncfg.MODEL.POINT_HEAD.FC_DIM = 256\ncfg.MODEL.POINT_HEAD.NUM_FC = 3\ncfg.MODEL.POINT_HEAD.CLS_AGNOSTIC_MASK = False\ncfg.MODEL.POINT_HEAD.COARSE_PRED_EACH_LAYER = True\ncfg.MODEL.POINT_HEAD.IN_FEATURES = [\"p2\"]\ncfg.MODEL.POINT_HEAD.TRAIN_NUM_POINTS = 2048 \ncfg.MODEL.POINT_HEAD.OVERSAMPLE_RATIO = 3\ncfg.MODEL.POINT_HEAD.IMPORTANCE_SAMPLE_RATIO = 0.75\n\ncfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.2, 0.5, 1.0, 2.0, 5.0]]\n\n# Parametri di Training\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.INPUT.MASK_FORMAT = \"bitmask\"\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.SOLVER.BASE_LR = 0.0025\ncfg.SOLVER.MAX_ITER = 45000 \ncfg.SOLVER.STEPS = (34000, 41000)\ncfg.SOLVER.CHECKPOINT_PERIOD = 5000 \n\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256 \ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1     \ncfg.MODEL.POINT_HEAD.NUM_CLASSES = 1    \n\ncfg.INPUT.MIN_SIZE_TRAIN = (700,750,800)\ncfg.INPUT.MAX_SIZE_TRAIN = 1333\ncfg.INPUT.MIN_SIZE_TEST = 700\ncfg.INPUT.MAX_SIZE_TEST = 1333\n\n# Output\ncfg.OUTPUT_DIR = \"/kaggle/working/output_models\"\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n\n# --- AVVIO DEL TRAINING ---\ntrainer = CustomTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n!git clone https://github.com/facebookresearch/detectron2.git detectron2_repo\n\n!python -m pip install -e detectron2_repo\n\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CODICE FORNITOCI PER LA VALUTAZIONE AUTOMATICA DELL'LDS SCORE","metadata":{}},{"cell_type":"markdown","source":"## 5. Valutazione: Line Detection Score (LDS)\nIl successo del modello non è misurato solo tramite mAP, ma tramite l'**LDS**, che pondera la qualità della segmentazione con la precisione geometrica dell'angolo:\n$$LDS = mAP + mAR + 2 \\cdot e^{-0.12 \\cdot \\Delta\\theta}$$","metadata":{}},{"cell_type":"code","source":"from pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom pycocotools import mask as coco_mask\nimport json\nimport numpy as np\nimport cv2\n\ndef evaluate_segmentation(gt_json_path, pred_json_path, check_cable_class=False):\n    # Load ground truth\n    coco_gt = COCO(gt_json_path)\n\n    # Load predictions\n    with open(pred_json_path, 'r') as f:\n        predictions = json.load(f)\n\n    # Load results into COCO results structure\n    coco_res = coco_gt.loadRes(predictions)\n\n    # Create COCOeval object\n    coco_eval = COCOeval(coco_gt, coco_res, 'segm')\n    if check_cable_class:\n        coco_eval.params.catIds = [0]  # id of the cable class\n\n    # Run evaluation\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n\n    avg_p50 = coco_eval.stats[1]\n    avg_r50 = coco_eval.stats[7]\n    return avg_p50, avg_r50","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def combined_analysis(gt_annotation_file, prediction_file):\n    # Load ground truth data\n    with open(gt_annotation_file, 'r') as f:\n        gt_data = json.load(f)\n    # Load prediction data\n    with open(prediction_file, 'r') as f:\n        pred_data = json.load(f)\n    # Group GT lines by image id\n    gt_lines_by_image = defaultdict(list)\n    for ann in gt_data['annotations']:\n        image_id = ann['image_id']\n        if 'polar_coordinates' in ann:\n            lines = [(coord['rho'], coord['theta']) for coord in ann['polar_coordinates']]\n            gt_lines_by_image[image_id].extend(lines)\n        else:\n            raise RuntimeError(f'no polar coord for image id {image_id}')\n    # Group predictions by image id\n    pred_by_image = defaultdict(list)\n    for pred in pred_data:\n        pred_by_image[pred['image_id']].append(pred)\n    angle_diffs = []\n    rho_diffs = []\n    \n    def theta_diff(theta_pred, theta_gt):\n        t = min(abs(theta_pred - theta_gt), np.pi - abs(theta_pred - theta_gt))\n        return np.exp(-.12 * t)\n    \n    def polygons_to_mask(polygons, shape):\n        mask = np.zeros(shape, dtype=np.uint8)\n        for polygon in polygons:\n            pts = np.array(polygon).reshape((-1, 2)).astype(np.int32)\n            cv2.fillPoly(mask, [pts], color=255)\n        return mask\n    \n    def compute_iou(mask1, mask2):\n        intersection = np.logical_and(mask1, mask2).sum()\n        union = np.logical_or(mask1, mask2).sum()\n        return intersection / union if union > 0 else 0\n    \n    total_matches = 0\n    total_gt_lines = 0\n    total_pred_lines = 0\n    \n    for image_info in tqdm(gt_data['images']):\n        image_id = image_info['id']\n        height, width = image_info['height'], image_info['width']\n        \n        # Load predictions for this image\n        pred_masks = []\n        pred_lines = []\n        for pred in pred_by_image.get(image_id, []):\n            seg = pred['segmentation']\n            if isinstance(seg, list):\n                mask_poly = polygons_to_mask(seg, (height, width))\n                pred_masks.append(mask_poly)\n            elif isinstance(seg, dict) and 'counts' in seg and 'size' in seg:\n                mask_rle = coco_mask.decode(seg)\n                if mask_rle.ndim == 3:\n                    mask_rle = mask_rle[:, :, 0]\n                mask_rle = (mask_rle * 255).astype(np.uint8)\n                pred_masks.append(mask_rle)\n            else:\n                raise RuntimeError(f'[SEGM] unsupported format for image id {image_id}')\n            \n            # Extract predicted line if exists\n            if 'lines' in pred and len(pred['lines']) == 2:\n                rho, theta = pred['lines']\n                rho = np.abs(rho / np.sqrt(height**2 + width**2))\n                pred_lines.append((rho, theta))\n            else:\n                pred_lines.append(None)\n        \n        # Load ground truth masks for this image\n        gt_masks = []\n        gt_lines = []\n        for ann in gt_data['annotations']:\n            if ann['image_id'] == image_id:\n                seg = ann['segmentation']\n                if isinstance(seg, list):\n                    mask_poly = polygons_to_mask(seg, (height, width))\n                    gt_masks.append(mask_poly)\n                elif isinstance(seg, dict) and 'counts' in seg and 'size' in seg:\n                    mask_rle = coco_mask.decode(seg)\n                    if mask_rle.ndim == 3:\n                        mask_rle = mask_rle[:, :, 0]\n                    mask_rle = (mask_rle * 255).astype(np.uint8)\n                    gt_masks.append(mask_rle)\n                else:\n                    raise RuntimeError(f'[GT] unsupported format for image id {image_id}')\n                \n                # Extract GT line\n                if 'polar_coordinates' in ann and len(ann['polar_coordinates']) > 0:\n                    rho, theta = ann['polar_coordinates'][0]['rho'], ann['polar_coordinates'][0]['theta']\n                    rho = np.abs(rho / np.sqrt(height**2 + width**2))\n                    gt_lines.append((rho, theta))\n                else:\n                    gt_lines.append(None)\n        \n        # Detect the matching mask by IoU\n        matched_gt = set()\n        for pred_idx, pred_mask in enumerate(pred_masks):\n            best_iou = 0\n            best_gt_idx = -1\n            \n            for gt_idx, gt_mask in enumerate(gt_masks):\n                if gt_idx in matched_gt:\n                    continue\n                iou = compute_iou(pred_mask, gt_mask)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = gt_idx\n            \n            # Consider it a match if IoU > threshold (e.g., 0.5)\n            #if best_iou > 0.5:\n            if best_gt_idx >= 0:\n                matched_gt.add(best_gt_idx)\n                total_matches += 1\n                \n                # Compute the rho_diff and theta_diff if both lines exist\n                pred_line = pred_lines[pred_idx]\n                gt_line = gt_lines[best_gt_idx]\n                \n                if pred_line is not None and gt_line is not None:\n                    rho_pred, theta_pred = pred_line\n                    rho_gt, theta_gt = gt_line\n                    \n                    rho_diffs.append(abs(rho_pred - rho_gt))\n                    angle_diffs.append(theta_diff(theta_pred, theta_gt))\n        \n        # Count matching and not matching lines\n        total_gt_lines += len(gt_masks)\n        total_pred_lines += len(pred_masks)\n    \n    print(f\"Total GT lines: {total_gt_lines}\")\n    print(f\"Total predicted lines: {total_pred_lines}\")\n    print(f\"Total matches: {total_matches}\")\n    print(f\"Lines with coordinate differences computed: {len(rho_diffs)}\")\n    \n    if len(rho_diffs) == 0:\n        return 0, 0\n    \n    return np.mean(rho_diffs), np.mean(angle_diffs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\n\nORIGINAL_JSON_PATH = \"/kaggle/input/json-annotazioni/test.json\"\nFIXED_JSON_PATH = \"/kaggle/working/test_fixed.json\"\n\n\nwith open(ORIGINAL_JSON_PATH, 'r') as f:\n    data = json.load(f)\n\n# Controllo e aggiunta del campo info\nif \"info\" not in data:\n    print(\"Campo 'info' mancante. Aggiunta in corso...\")\n    data[\"info\"] = {\n        \"description\": \"Cable Dataset Test Set\",\n        \"url\": \"http://kaggle.com\",\n        \"version\": \"1.0\",\n        \"year\": 2024,\n        \"contributor\": \"User\",\n        \"date_created\": \"2024-01-17\"\n    }\nelse:\n    print(\"Il campo 'info' esiste già.\")\n\nwith open(FIXED_JSON_PATH, 'w') as f:\n    json.dump(data, f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_line_detection_score(gt_json_path, pred_json_path):\n\n    avg_p50, avg_r50 = evaluate_segmentation(gt_json_path, pred_json_path)\n    rho_diff, angle_diff = combined_analysis(gt_json_path, pred_json_path)\n\n    print(f'{avg_p50=}, {avg_r50=}, {angle_diff=}')\n\n    lds = avg_p50 + avg_r50 + 2 * angle_diff\n    print(f'LDS = {lds}')\n    return lds\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\nimport numpy as np\nimport cv2\nimport json\nimport os\nimport math\nimport random\nimport contextlib\nimport io\nimport sys\nfrom tqdm import tqdm\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.data.datasets import register_coco_instances \nfrom detectron2.projects import point_rend\nimport pycocotools.mask as mask_util\n\n# Percorsi\nTEST_IMG_DIR = \"/kaggle/input/test-cv\"\nTEST_JSON_PATH = \"/kaggle/working/test_fixed.json\" \nWEIGHTS_PATH = \"/kaggle/input/pesi-pointrend/pesi_finali.pth\"\nOUTPUT_JSON = \"MARTUCCI_271316_ZAPPIA_268784.json\" \n\n# 2. FUNZIONI DI UTILITÀ\ndef binary_mask_to_rle(binary_mask):\n    rle = mask_util.encode(np.asfortranarray(binary_mask.astype(np.uint8)))\n    rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n    return rle\n\ndef get_polar_line(binary_mask):\n    \"\"\"Calcola Rho e Theta dalla maschera.\"\"\"\n    y_coords, x_coords = np.where(binary_mask)\n    if len(x_coords) < 5: return [0.0, 0.0]\n\n    pts = np.column_stack((x_coords, y_coords))\n    try:\n        [vx, vy, x0, y0] = cv2.fitLine(pts, cv2.DIST_L2, 0, 0.01, 0.01)\n        vx, vy, x0, y0 = vx[0], vy[0], x0[0], y0[0]\n        nx, ny = -vy, vx\n        theta = math.atan2(ny, nx)\n        rho = x0 * nx + y0 * ny\n        if rho < 0:\n            rho = -rho\n            theta += math.pi\n        theta = theta % (2 * math.pi)\n        return [float(rho), float(theta)]\n    except:\n        return [0.0, 0.0]\n\ndef get_bbox_from_mask(mask):\n    \"\"\"Ricalcola la BBox stretta attorno alla maschera.\"\"\"\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n    \n    if not np.any(rows) or not np.any(cols):\n        return None \n        \n    ymin, ymax = np.where(rows)[0][[0, -1]]\n    xmin, xmax = np.where(cols)[0][[0, -1]]\n    \n    # Formato COCO: [x_min, y_min, width, height]\n    return [float(xmin), float(ymin), float(xmax - xmin + 1), float(ymax - ymin + 1)]\n\n\n# Configurazione\ncfg = get_cfg()\npoint_rend.add_pointrend_config(cfg)\n\nconfig_path = \"detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\"\ncfg.merge_from_file(config_path)\ncfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.2, 0.5, 1.0, 2.0, 5.0]]\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  \ncfg.MODEL.POINT_HEAD.NUM_CLASSES = 1\ncfg.MODEL.WEIGHTS = WEIGHTS_PATH\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\ncfg.INPUT.MASK_FORMAT = \"bitmask\"\ncfg.INPUT.MIN_SIZE_TEST = 700\ncfg.INPUT.MAX_SIZE_TEST = 700\ncfg.MODEL.DEVICE = \"cuda\"\n\n# Impostazione Parametri Ottimali\ncfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.7\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.01\n\npredictor = DefaultPredictor(cfg)\n\n# Dataset\nDATASET_NAME = \"final_test_dataset\"\nif DATASET_NAME in DatasetCatalog.list(): DatasetCatalog.remove(DATASET_NAME)\nregister_coco_instances(DATASET_NAME, {}, TEST_JSON_PATH, TEST_IMG_DIR)\ndataset_dicts = DatasetCatalog.get(DATASET_NAME)\n\nfinal_results = []\n\nfor d in tqdm(dataset_dicts, desc=\"Processing Images\"):\n    img = cv2.imread(d[\"file_name\"])\n    outputs = predictor(img)\n    instances = outputs[\"instances\"].to(\"cpu\")\n    \n    for i in range(len(instances)):\n        # 1. Maschera Originale\n        original_mask = instances.pred_masks[i].numpy()\n        score = float(instances.scores[i])\n                 \n        bbox_new = get_bbox_from_mask(original_mask)\n\n        if bbox_new is None:\n            continue\n            \n        # 2. Calcolo Parametri sulla maschera \n        rle = binary_mask_to_rle(original_mask)\n        line_params = get_polar_line(original_mask)\n        area = float(np.sum(original_mask))\n        \n        res = {\n            \"image_id\": d[\"image_id\"],\n            \"category_id\": 0,\n            \"bbox\": bbox_new,     \n            \"segmentation\": rle,  \n            \"score\": score,\n            \"lines\": line_params, \n            \"area\": area,\n            \"id\": random.randint(1, 99999999)\n        }\n        final_results.append(res)\n\n# Salvataggio\nwith open(OUTPUT_JSON, \"w\") as f:\n    json.dump(final_results, f)\n\nprint(f\"Predizioni salvate in: {OUTPUT_JSON}\")\n\n\n\nlds_score = compute_line_detection_score(\"/kaggle/working/test_fixed.json\", OUTPUT_JSON)\n\nprint(\"=\"*50)\nprint(f\"LDS SCORE FINALE: {lds_score}\")\nprint(\"=\"*50)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Post-Processing Geometrico: Correzione del Bias\nAttraverso una **Grid Search** sul training set, abbiamo identificato un errore sistematico di disallineamento geometrico (shift). \nIl codice seguente analizza le maschere predette rispetto alla Ground Truth per calcolare lo shift ottimale $(dx, dy)$ che massimizza l'IoU.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport pycocotools.mask as mask_util\nfrom tqdm import tqdm\nfrom detectron2.data import DatasetCatalog\n\n\n# 1. FUNZIONI DI SUPPORTO\ndef annotation_to_mask(segmentation, h, w):\n    \"\"\"\n    Converte l'annotazione COCO (poligono o RLE) in maschera binaria numpy.\n    Sostituisce GenericMask di Detectron2.\n    \"\"\"\n    if isinstance(segmentation, list):\n        # Caso Poligono: [[x1, y1, x2, y2, ...], [...]]\n        # frPyObjects si aspetta una lista di liste\n        rles = mask_util.frPyObjects(segmentation, h, w)\n        rle = mask_util.merge(rles)\n    elif isinstance(segmentation, dict):\n        # Caso RLE standard COCO\n        rle = segmentation\n    else:\n        return np.zeros((h, w), dtype=bool)\n\n    # Decodifica RLE in maschera binaria (0 e 1)\n    m = mask_util.decode(rle)\n    return m.astype(bool)\n\ndef calculate_mask_iou(mask1, mask2):\n    \"\"\"Calcola IoU veloce tra due maschere booleane.\"\"\"\n    intersection = (mask1 & mask2).sum()\n    union = (mask1 | mask2).sum()\n    if union == 0: return 0.0\n    return intersection / union\n\ndef apply_shift_and_morph(mask, dx, dy, morph_op=0):\n    \"\"\"Applica shift geometrico e morfologia.\"\"\"\n    m_uint = mask.astype(np.uint8)\n    h, w = mask.shape\n    shifted = np.zeros_like(m_uint)\n    \n    # 1. Shift sicuro con slicing\n    src_y_start, src_y_end = max(0, -dy), min(h, h - dy)\n    src_x_start, src_x_end = max(0, -dx), min(w, w - dx)\n    dst_y_start, dst_y_end = max(0, dy), min(h, h + dy)\n    dst_x_start, dst_x_end = max(0, dx), min(w, w + dx)\n    \n    shifted[dst_y_start:dst_y_end, dst_x_start:dst_x_end] = \\\n        m_uint[src_y_start:src_y_end, src_x_start:src_x_end]\n        \n    # 2. Morfologia\n    if morph_op != 0:\n        kernel = np.ones((3,3), np.uint8)\n        if morph_op > 0:\n            shifted = cv2.dilate(shifted, kernel, iterations=morph_op)\n        else:\n            shifted = cv2.erode(shifted, kernel, iterations=abs(morph_op))\n            \n    return shifted.astype(bool)\n\n\n# 2. MOTORE DI CALIBRAZIONE\ndef run_full_calibration(predictor, dataset_name):\n    dataset_dicts = DatasetCatalog.get(dataset_name)\n    print(f\"Avvio calibrazione sulle {len(dataset_dicts)} immagini di train.\")\n    \n    valid_pairs = [] # Lista di tuple (gt_mask, pred_mask)\n    \n    print(\"Fase 1/2: Raccolta coppie Predizione-GT...\")\n    for d in tqdm(dataset_dicts):\n        img = cv2.imread(d[\"file_name\"])\n        h, w = d[\"height\"], d[\"width\"]\n        \n        # Inferenza \n        outputs = predictor(img)\n        pred_instances = outputs[\"instances\"].to(\"cpu\")\n        \n        if len(pred_instances) == 0 or \"annotations\" not in d:\n            continue\n            \n        # Prepara GT Masks \n        gt_masks = []\n        for anno in d[\"annotations\"]:\n            m = annotation_to_mask(anno[\"segmentation\"], h, w)\n            gt_masks.append(m)\n        \n        if not gt_masks: continue\n        gt_masks = np.array(gt_masks) # [N_GT, H, W]\n\n        # Match\n        pred_masks_np = pred_instances.pred_masks.numpy()\n        scores = pred_instances.scores.numpy()\n        \n        for i, pred_mask in enumerate(pred_masks_np):\n            if scores[i] < 0.1: continue \n            \n            # Vectorized IoU \n            intersections = np.logical_and(gt_masks, pred_mask).sum(axis=(1,2))\n            unions = np.logical_or(gt_masks, pred_mask).sum(axis=(1,2))\n            ious = intersections / (unions + 1e-6)\n            \n            best_gt_idx = np.argmax(ious)\n            max_iou = ious[best_gt_idx]\n            \n            if max_iou > 0.1:\n                valid_pairs.append((gt_masks[best_gt_idx], pred_mask))\n\n    n_pairs = len(valid_pairs)\n    print(f\"Fase 1 completata. Trovate {n_pairs} coppie valide.\")\n    \n    if n_pairs == 0:\n        print(\"Nessuna coppia valida trovata.\")\n        return 0, 0, 0\n\n    # FASE 2: GRID SEARCH \n    print(\"Fase 2/2: Grid Search parametri ottimali...\")\n    \n    x_range = range(-3, 4) \n    y_range = range(-3, 4)\n    morph_range = [-1, 0, 1] \n    \n    results = {}\n    total_combinations = len(x_range) * len(y_range) * len(morph_range)\n    \n    with tqdm(total=total_combinations) as pbar:\n        for dx in x_range:\n            for dy in y_range:\n                for morph in morph_range:\n                    cum_iou = 0.0\n                    for gt, pred in valid_pairs:\n                        mod_pred = apply_shift_and_morph(pred, dx, dy, morph)\n                        cum_iou += calculate_mask_iou(mod_pred, gt)\n                    \n                    results[(dx, dy, morph)] = cum_iou / n_pairs\n                    pbar.update(1)\n\n    # RISULTATI \n    best_params = max(results, key=results.get)\n    best_iou = results[best_params]\n    base_iou = results[(0,0,0)]\n\n    print(\"\\n\" + \"═\"*50)\n    print(f\"RISULTATI CALIBRAZIONE\")\n    print(\"═\"*50)\n    print(f\"IoU Medio Iniziale:    {base_iou:.5f}\")\n    print(f\"IoU Medio Ottimizzato: {best_iou:.5f}\")\n    print(f\"Miglioramento:         +{(best_iou - base_iou)*100:.2f}%\")\n    print(\"─\"*50)\n    print(f\"FARE QUESTE OPERAZIONI:\")\n    print(f\"Shift X (dx): {best_params[0]}\")\n    print(f\"Shift Y (dy): {best_params[1]}\")\n    print(f\"Morfologia:   {best_params[2]} (1=dilata, -1=erodi, 0=nulla)\")\n    print(\"═\"*50)\n    \n    return best_params\n\n# 1. CONFIGURAZIONE E REGISTRAZIONE DATASET\nTRAIN_IMG_DIR = \"/kaggle/input/train-cv\"\nTRAIN_JSON_PATH = \"/kaggle/input/json-annotazioni/train.json\"\n\nDATASET_NAME = \"my_test_dataset_final\"\ntry:\n    register_coco_instances(DATASET_NAME, {}, TRAIN_JSON_PATH, TRAIN_IMG_DIR)\nexcept AssertionError:\n    pass # Già registrato\n\nbest_dx, best_dy, best_morph = run_full_calibration(predictor, DATASET_NAME)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Inferenza Finale e Generazione Output\nApplichiamo il modello sul Test Set con i parametri ottimizzati:\n* **Score Threshold: 0.01** (Massimizziamo la Recall, fondamentale per il punteggio LDS).\n* **NMS Threshold: 0.7** (Evitiamo di sopprimere cavi paralleli o incrociati).\n* **Shift Correction (+1, +1):** Correzione del bias geometrico rilevato.\n\nInfine, calcoliamo le coordinate polari tramite regressione `cv2.fitLine` sui pixel della maschera.","metadata":{}},{"cell_type":"code","source":"import detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\nimport numpy as np\nimport cv2\nimport json\nimport os\nimport math\nimport random\nimport contextlib\nimport io\nimport sys\nfrom tqdm import tqdm\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.projects import point_rend\nimport pycocotools.mask as mask_util\n\n# 1. PARAMETRI MIGLIORI IN SEGUITO ALLA GRID SEARCH\nBEST_NMS = 0.7   \nBEST_SCORE = 0.01 \n\n# Percorsi\nTEST_IMG_DIR = \"/kaggle/input/test-cv\"\nTEST_JSON_PATH = \"/kaggle/working/test_fixed.json\" # Quello corretto con il campo info\nWEIGHTS_PATH = \"/kaggle/input/pesi-pointrend/pesi_finali.pth\"\nOUTPUT_SHIFTED_JSON = \"MARTUCCI_271316_ZAPPIA_268784.json\"\n\n# 2. FUNZIONI DI UTILITÀ (SHIFT & RICALCOLO)\n\ndef binary_mask_to_rle(binary_mask):\n    rle = mask_util.encode(np.asfortranarray(binary_mask.astype(np.uint8)))\n    rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n    return rle\n\ndef get_polar_line(binary_mask):\n    \"\"\"Calcola Rho e Theta dalla maschera.\"\"\"\n    y_coords, x_coords = np.where(binary_mask)\n    if len(x_coords) < 5: return [0.0, 0.0]\n\n    pts = np.column_stack((x_coords, y_coords))\n    try:\n        [vx, vy, x0, y0] = cv2.fitLine(pts, cv2.DIST_L2, 0, 0.01, 0.01)\n        vx, vy, x0, y0 = vx[0], vy[0], x0[0], y0[0]\n        nx, ny = -vy, vx\n        theta = math.atan2(ny, nx)\n        rho = x0 * nx + y0 * ny\n        if rho < 0:\n            rho = -rho\n            theta += math.pi\n        theta = theta % (2 * math.pi)\n        return [float(rho), float(theta)]\n    except:\n        return [0.0, 0.0]\n\ndef apply_shift_and_recalc(original_mask, dx=1, dy=1):\n    \"\"\"\n    Applica shift geometrico alla maschera e restituisce la maschera spostata.\n    \"\"\"\n    h, w = original_mask.shape\n    shifted_mask = np.zeros_like(original_mask)\n    \n    # Logica di Shift Numpy Slicing:\n    # Copia src[:-dy, :-dx] in dst[dy:, dx:]\n    # Gestisce i bordi evitando errori di indice\n    src_y_start, src_y_end = 0, h - dy\n    src_x_start, src_x_end = 0, w - dx\n    dst_y_start, dst_y_end = dy, h\n    dst_x_start, dst_x_end = dx, w\n    \n    shifted_mask[dst_y_start:dst_y_end, dst_x_start:dst_x_end] = \\\n        original_mask[src_y_start:src_y_end, src_x_start:src_x_end]\n        \n    return shifted_mask\n\ndef get_bbox_from_mask(mask):\n    \"\"\"Ricalcola la BBox stretta attorno alla maschera (necessario dopo lo shift).\"\"\"\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n    \n    if not np.any(rows) or not np.any(cols):\n        return None # Maschera vuota dopo lo shift (es. uscita dall'immagine)\n        \n    ymin, ymax = np.where(rows)[0][[0, -1]]\n    xmin, xmax = np.where(cols)[0][[0, -1]]\n    \n    # Formato COCO: [x_min, y_min, width, height]\n    return [float(xmin), float(ymin), float(xmax - xmin + 1), float(ymax - ymin + 1)]\n\n\n# Configurazione\ncfg = get_cfg()\npoint_rend.add_pointrend_config(cfg)\n\nconfig_path = \"detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\"\ncfg.merge_from_file(config_path)\ncfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.2, 0.5, 1.0, 2.0, 5.0]]\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  \ncfg.MODEL.POINT_HEAD.NUM_CLASSES = 1\ncfg.MODEL.WEIGHTS = WEIGHTS_PATH\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\ncfg.INPUT.MASK_FORMAT = \"bitmask\"\ncfg.INPUT.MIN_SIZE_TEST = 700\ncfg.INPUT.MAX_SIZE_TEST = 700\ncfg.MODEL.DEVICE = \"cuda\"\n\n# Impostazione Parametri Ottimali\ncfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = BEST_NMS\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = BEST_SCORE\n\npredictor = DefaultPredictor(cfg)\n\n# Dataset\nDATASET_NAME = \"final_test_ds\"\nif DATASET_NAME in DatasetCatalog.list(): DatasetCatalog.remove(DATASET_NAME)\nregister_coco_instances(DATASET_NAME, {}, TEST_JSON_PATH, TEST_IMG_DIR)\ndataset_dicts = DatasetCatalog.get(DATASET_NAME)\n\nfinal_results = []\n\nfor d in tqdm(dataset_dicts, desc=\"Processing Images\"):\n    img = cv2.imread(d[\"file_name\"])\n    outputs = predictor(img)\n    instances = outputs[\"instances\"].to(\"cpu\")\n    \n    for i in range(len(instances)):\n        # 1. Maschera Originale\n        original_mask = instances.pred_masks[i].numpy()\n        score = float(instances.scores[i])\n        \n        # 2. APPLICAZIONE SHIFT (+1, +1)\n        shifted_mask = apply_shift_and_recalc(original_mask, dx=1, dy=1)\n        \n        # 3. Ricalcolo BBox (Fondamentale: la box deve seguire la maschera)\n        bbox_new = get_bbox_from_mask(shifted_mask)\n        \n        # Se lo shift ha fatto uscire la maschera dall'immagine, saltiamo\n        if bbox_new is None:\n            continue\n            \n        # 4. Ricalcolo Parametri sulla maschera spostata\n        rle = binary_mask_to_rle(shifted_mask)\n        line_params = get_polar_line(shifted_mask)\n        area = float(np.sum(shifted_mask))\n        \n        res = {\n            \"image_id\": d[\"image_id\"],\n            \"category_id\": 0,\n            \"bbox\": bbox_new,     # Box aggiornata\n            \"segmentation\": rle,  # RLE aggiornato\n            \"score\": score,\n            \"lines\": line_params, # Linea aggiornata\n            \"area\": area,\n            \"id\": random.randint(1, 99999999)\n        }\n        final_results.append(res)\n\n# Salvataggio\nwith open(OUTPUT_SHIFTED_JSON, \"w\") as f:\n    json.dump(final_results, f)\n\nprint(f\"Predizioni shiftate salvate in: {OUTPUT_SHIFTED_JSON}\")\n\n# 3. VALUTAZIONE LDS\n\ntry:\n    with contextlib.redirect_stdout(io.StringIO()):\n        lds_score = compute_line_detection_score(TEST_JSON_PATH, OUTPUT_SHIFTED_JSON)\n\n    print(\"=\"*50)\n    print(f\"LDS SCORE FINALE (SHIFT +1,+1): {lds_score}\")\n    print(\"=\"*50)\n\nexcept Exception as e:\n    print(f\"Errore nel calcolo dello score: {e}\")\n    compute_line_detection_score(TEST_JSON_PATH, OUTPUT_SHIFTED_JSON)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}